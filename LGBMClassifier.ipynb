{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d02987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import lightgbm as lgb\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import random\n",
    "\n",
    "readRDS = robjects.r['readRDS']\n",
    "df = readRDS('do_class_ohe.Rds')\n",
    "df = pandas2ri.rpy2py_dataframe(df)\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "df_clean = clean_dataset(df)\n",
    "\n",
    "X = df_clean.iloc[:, 2:-1].values\n",
    "y = df_clean.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)\n",
    "\n",
    "def learning_rate_010_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_010_decay_power_0995(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_005_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.05\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [(X_test,y_test)],\n",
    "            'eval_names': ['valid'],\n",
    "            #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.ðŸ˜Ž, \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)\n",
    "\n",
    "#gs.fit(X_train, y_train, **fit_params)\n",
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))\n",
    "\n",
    "opt_parameters = {'colsample_bytree': 0.9522, 'min_child_samples': 111, 'min_child_weight': 0.01, 'num_leaves': 38, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.30293}\n",
    "\n",
    "clf_final = lgb.LGBMClassifier(**gs.best_estimator_.get_params())\n",
    "\n",
    "clf_final = lgb.LGBMClassifier(**clf.get_params())\n",
    "\n",
    "clf_final.set_params(**opt_parameters)\n",
    "\n",
    "random.seed(997)\n",
    "clf_final.fit(X_train, y_train, **fit_params, callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])\n",
    "\n",
    "probs = clf_final.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "Roc_Analysis(y_test, preds)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', random_state=997)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', random_state=0).fit(X_train, y_train)\n",
    "model.coef_\n",
    "\n",
    "probs = model.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "Roc_Analysis(y_test, preds)\n",
    "\n",
    "del df['BKG_SALES_DATE_MONTH.12']\n",
    "del df['KG_SALES_DATE_WEEKDAY.7']\n",
    "del df['BKG_BOOKING_WINDOW_D']\n",
    "del df['BKG_STAY_LENGTH_GROUPED.Short']\n",
    "del df['BKG_FLIGHT_COUPONS']\n",
    "del df['BKG_SALES_CHANNEL_grouped.1']\n",
    "del df['TKT_CURRENCY_fin.USD']\n",
    "del df['BKG_SALES_DATE_MONTH.12']\n",
    "del df['BKG_SALES_DATE_MONTH.12']\n",
    "del df['BKG_SALES_DATE_MONTH.12']\n",
    "del df['BKG_SALES_DATE_MONTH.12']\n",
    "del df['BKG_SALES_DATE_MONTH.12']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
